{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.7, min_sigma=0.4, decay_period=600_000):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = max_sigma\n",
    "        self.max_sigma = max_sigma\n",
    "        self.min_sigma = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim = action_space\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "    def get_noise(self, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        decaying = float(float(t) / self.decay_period)\n",
    "        self.sigma = max(self.sigma - (self.max_sigma - self.min_sigma) * min(1.0, decaying), self.min_sigma)\n",
    "        print('sigma:', self.sigma, 'state:', ou_state)\n",
    "        return ou_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(action_space=2, max_sigma=0.9, min_sigma=0.1, decay_period=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: 0.8904 state: [ 0.42474944 -0.63381683]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.42474944, -0.63381683])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.get_noise(t=60_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_utils import GoalManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mObstacle name: wall_outler, base pose: (0.0, 0.0, 0.0)\u001b[0m\n",
      "Coordinates: [[[11.5, -8.425], [11.5, -11.575], [-11.5, -11.575], [-11.5, -8.425]], [[21.5, 1.575], [21.5, -1.575], [-1.5, -1.575], [-1.5, 1.575]], [[11.5, 11.575], [11.5, 8.425], [-11.5, 8.425], [-11.5, 11.575]], [[1.5, 1.575], [1.5, -1.575], [-21.5, -1.575], [-21.5, 1.575]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GM = GoalManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([979.3260, 541.2661, 100.7525, 492.2729, 200.7144])\n",
      "tensor([0.7879, 0.4355, 0.0811, 0.3961, 0.1615])\n",
      "tensor([0.7879, 0.4355, 0.0811, 0.3961, 0.1615])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "v = torch.rand(5) * 1000\n",
    "v_1 = v.clone()\n",
    "v.requires_grad_(True)\n",
    "v_1.requires_grad_(True)\n",
    "\n",
    "loss = 1/2 * torch.sum(v_1 * v_1 + v * v)\n",
    "# Here grads of loss w.r.t v and v_1 should be v and v_1 respectively\n",
    "loss.backward()\n",
    "\n",
    "# Clip grads of v_1\n",
    "torch.nn.utils.clip_grad_norm_(v_1, max_norm=1.0, norm_type=2)\n",
    "\n",
    "print(v.grad)\n",
    "print(v_1.grad)\n",
    "print(v.grad / torch.norm(v.grad, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([979.3260, 541.2661, 100.7525, 492.2729, 200.7144])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "v_2 = v_1.clone()\n",
    "\n",
    "v_3 = torch.tanh(v_2)\n",
    "\n",
    "v_2.requires_grad_(True)\n",
    "v_3.requires_grad_(True)\n",
    "\n",
    "loss2 = 1/2 * torch.sum(v_2 * v_2 + v_3 * v_3)\n",
    "\n",
    "# Retain grad for v_2 and v_3\n",
    "v_2.retain_grad()\n",
    "v_3.retain_grad()\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(v_2.grad)\n",
    "print(v_3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "    name,           # Name of the network\n",
    "    state_size,     # Number of input neurons\n",
    "    action_size,    # Number of output neurons\n",
    "    hidden_size,     # Number of neurons in hidden layers\n",
    "    ):\n",
    "        super(Actor, self).__init__()\n",
    "        self.name = name\n",
    "        self.iteration = 0\n",
    "\n",
    "        # Layer Definition\n",
    "        self.fa1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fa2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fa3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fa4 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.is_leaf:\n",
    "                print(f\"Parameter {name} is a leaf tensor.\")\n",
    "\n",
    "        # Initialize weights\n",
    "        # Using Kaiming initialization\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m: torch.nn.Module):\n",
    "        # Initialize the weights of the network\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Kaiming He initialization\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, states, visualize=False):\n",
    "        # Forward pass\n",
    "        x1 = torch.relu(self.fa1(states))\n",
    "        x2 = torch.relu(self.fa2(x1))\n",
    "        x3 = torch.relu(self.fa3(x2))\n",
    "        action = torch.tanh(self.fa4(x3))\n",
    "\n",
    "        return action\n",
    "    \n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "    name,           # Name of the network \n",
    "    state_size,     # Number of input neurons\n",
    "    action_size,    # Number of output neurons\n",
    "    hidden_size,     # Number of neurons in hidden layers\n",
    "    ):\n",
    "        super(Critic, self).__init__()\n",
    "        self.name = name\n",
    "        self.iteration = 0\n",
    "\n",
    "        # Q1 Architecture\n",
    "        self.l01 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.l02 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l03 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l04 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Q2 Architecture\n",
    "        self.l11 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.l12 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l13 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l14 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.is_leaf:\n",
    "                print(f\"Parameter {name} is a leaf tensor.\")\n",
    "\n",
    "        # Initialize weights\n",
    "        # Using Kaiming initialization\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m: torch.nn.Module):\n",
    "        # Initialize the weights of the network\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Kaiming He initialization\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, states, actions, visualize=False) -> torch.Tensor:\n",
    "        \n",
    "        # Concatenate the states and actions\n",
    "        sa = torch.cat((states, actions), dim=1)\n",
    "\n",
    "        # Q1 forward pass\n",
    "        x01 = torch.relu(self.l01(sa))\n",
    "        x02 = torch.relu(self.l02(x01))\n",
    "        x03 = torch.relu(self.l03(x02))\n",
    "        q1 = self.l04(x03)\n",
    "\n",
    "        # Q2 forward pass\n",
    "        x11 = torch.relu(self.l11(sa))\n",
    "        x12 = torch.relu(self.l12(x11))\n",
    "        x13 = torch.relu(self.l13(x12))\n",
    "        q2 = self.l14(x13)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1_forward(self, states, actions) -> torch.Tensor:\n",
    "        \n",
    "        # Concatenate the states and actions\n",
    "        sa = torch.cat((states, actions), dim=1)\n",
    "\n",
    "        # Q1 forward pass\n",
    "        x01 = torch.relu(self.l01(sa))\n",
    "        x02 = torch.relu(self.l02(x01))\n",
    "        x03 = torch.relu(self.l03(x02))\n",
    "        q1 = self.l04(x03)\n",
    "\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter fa1.weight is a leaf tensor.\n",
      "Parameter fa1.bias is a leaf tensor.\n",
      "Parameter fa2.weight is a leaf tensor.\n",
      "Parameter fa2.bias is a leaf tensor.\n",
      "Parameter fa3.weight is a leaf tensor.\n",
      "Parameter fa3.bias is a leaf tensor.\n",
      "Parameter fa4.weight is a leaf tensor.\n",
      "Parameter fa4.bias is a leaf tensor.\n",
      "Parameter l01.weight is a leaf tensor.\n",
      "Parameter l01.bias is a leaf tensor.\n",
      "Parameter l02.weight is a leaf tensor.\n",
      "Parameter l02.bias is a leaf tensor.\n",
      "Parameter l03.weight is a leaf tensor.\n",
      "Parameter l03.bias is a leaf tensor.\n",
      "Parameter l04.weight is a leaf tensor.\n",
      "Parameter l04.bias is a leaf tensor.\n",
      "Parameter l11.weight is a leaf tensor.\n",
      "Parameter l11.bias is a leaf tensor.\n",
      "Parameter l12.weight is a leaf tensor.\n",
      "Parameter l12.bias is a leaf tensor.\n",
      "Parameter l13.weight is a leaf tensor.\n",
      "Parameter l13.bias is a leaf tensor.\n",
      "Parameter l14.weight is a leaf tensor.\n",
      "Parameter l14.bias is a leaf tensor.\n"
     ]
    }
   ],
   "source": [
    "state_size = 10\n",
    "action_size = 2\n",
    "\n",
    "actor = Actor('actor', state_size, action_size, 256).to(device)\n",
    "critic = Critic('critic', state_size, action_size, 256).to(device)\n",
    "\n",
    "actor_optimizer = torch.optim.AdamW(actor.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_actor: -1.9114747047424316\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0235,  0.0241,  0.0167,  ...,  0.0114,  0.0203,  0.0095],\n",
      "        ...,\n",
      "        [-0.0205, -0.0210, -0.0146,  ..., -0.0100, -0.0177, -0.0083],\n",
      "        [ 0.0197,  0.0202,  0.0140,  ...,  0.0096,  0.0170,  0.0079],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0000,  0.0000,  0.0251,  0.0302,  0.0000,  0.0000, -0.0129,  0.0000,\n",
      "         0.0033,  0.0000,  0.0000,  0.0000,  0.0163, -0.0376,  0.0000,  0.0000,\n",
      "        -0.0197,  0.0000,  0.0000,  0.0000,  0.0000,  0.0151,  0.0000,  0.0000,\n",
      "         0.0108,  0.0224, -0.0230, -0.0256,  0.0000,  0.0000,  0.0000, -0.0099,\n",
      "         0.0169,  0.0000, -0.0261,  0.0000,  0.0000,  0.0255,  0.0047,  0.0367,\n",
      "         0.0000,  0.0000,  0.0000, -0.0001,  0.0000,  0.0125, -0.0030, -0.0212,\n",
      "         0.0188, -0.0011,  0.0445, -0.0096, -0.0373,  0.0000, -0.0506,  0.0267,\n",
      "        -0.0134,  0.0000,  0.0358, -0.0323, -0.0043,  0.0297,  0.0000,  0.0000,\n",
      "        -0.0200, -0.0073,  0.0044, -0.0356,  0.0000, -0.0180, -0.0173,  0.0000,\n",
      "         0.0000,  0.0000,  0.0161,  0.0000,  0.0000, -0.0053,  0.0051,  0.0220,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0276,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0393, -0.0198,  0.0000,  0.0119,  0.0085,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0264, -0.0015,\n",
      "         0.0000, -0.0101,  0.0000,  0.0000, -0.0035, -0.0044,  0.0248, -0.0061,\n",
      "         0.0000,  0.0000, -0.0120,  0.0000,  0.0068,  0.0031,  0.0051,  0.0139,\n",
      "         0.0000, -0.0172, -0.0241,  0.0000,  0.0243, -0.0006,  0.0181, -0.0147,\n",
      "         0.0235,  0.0000,  0.0232,  0.0025,  0.0013,  0.0000,  0.0000,  0.0089,\n",
      "         0.0000,  0.0129,  0.0000,  0.0000,  0.0000, -0.0141,  0.0000,  0.0000,\n",
      "         0.0105, -0.0120, -0.0181,  0.0000, -0.0133,  0.0000, -0.0225,  0.0055,\n",
      "        -0.0122,  0.0000,  0.0000,  0.0275,  0.0000,  0.0000,  0.0000,  0.0127,\n",
      "         0.0069,  0.0000,  0.0000,  0.0000,  0.0000, -0.0155,  0.0000,  0.0035,\n",
      "         0.0000,  0.0000,  0.0166,  0.0000,  0.0000,  0.0152, -0.0015,  0.0081,\n",
      "         0.0306,  0.0000,  0.0000,  0.0000,  0.0196, -0.0094,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0257,  0.0070,  0.0000,  0.0000,  0.0000,\n",
      "         0.0078,  0.0015,  0.0172,  0.0004, -0.0158, -0.0156, -0.0090,  0.0000,\n",
      "        -0.0199,  0.0227, -0.0321,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0428,  0.0000, -0.0007,  0.0000, -0.0088,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0145,  0.0000,  0.0000,  0.0000,\n",
      "         0.0147,  0.0000,  0.0000,  0.0128,  0.0000, -0.0161, -0.0339,  0.0000,\n",
      "        -0.0089,  0.0000,  0.0000,  0.0000,  0.0303,  0.0000, -0.0026,  0.0087,\n",
      "         0.0153,  0.0151, -0.0409,  0.0099, -0.0189,  0.0000, -0.0066,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0219,  0.0211,  0.0000],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0000,  0.0000,  0.0046,  ...,  0.0040,  0.0028,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0024,  ..., -0.0021, -0.0014, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0044,  ...,  0.0038,  0.0027,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0009,  ...,  0.0008,  0.0006,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0042,  ..., -0.0037, -0.0026, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0024,  ...,  0.0021,  0.0014,  0.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([ 1.3918e-02, -7.1874e-03,  1.3432e-02,  0.0000e+00,  6.4085e-03,\n",
      "         1.6132e-02, -8.9652e-04, -4.9091e-03,  2.5611e-02, -5.8522e-04,\n",
      "         1.6882e-02, -7.3331e-03, -3.8204e-02,  0.0000e+00,  8.9181e-03,\n",
      "         2.5686e-02,  1.1894e-02, -4.6243e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -5.4645e-03,  2.2496e-03, -3.9888e-04,  2.2996e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.3245e-03,  0.0000e+00,  0.0000e+00, -2.5349e-03,  0.0000e+00,\n",
      "         8.8417e-03, -1.1335e-03,  1.4486e-02,  0.0000e+00, -1.1879e-03,\n",
      "         7.9226e-05,  0.0000e+00, -7.3105e-03,  0.0000e+00,  3.9221e-03,\n",
      "         2.7633e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.9515e-02,  0.0000e+00,  0.0000e+00, -3.7249e-02,  4.4106e-03,\n",
      "         2.9012e-02,  0.0000e+00,  0.0000e+00, -5.6098e-03,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4451e-03, -1.5849e-02,\n",
      "         7.9584e-03,  0.0000e+00,  7.1193e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -8.7078e-03,  0.0000e+00,  9.3858e-03,  6.0202e-03, -2.3990e-03,\n",
      "         4.0886e-03,  0.0000e+00,  2.3723e-02,  8.7567e-03,  1.1076e-02,\n",
      "        -2.2995e-02,  0.0000e+00,  2.2161e-02,  0.0000e+00,  1.5752e-02,\n",
      "         2.0659e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -3.4552e-02, -6.9920e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         2.0916e-02,  2.0552e-02,  1.7351e-04, -3.3623e-03, -1.4965e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4021e-02,  2.9849e-02,\n",
      "        -8.7851e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -4.3594e-03, -2.3738e-04, -9.9841e-03,  6.2647e-03, -3.8484e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -5.8946e-03, -1.1210e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.4988e-02, -1.6046e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -3.2516e-02,  2.1001e-02,  0.0000e+00, -2.7429e-02,  1.4403e-02,\n",
      "         0.0000e+00, -2.3638e-02, -6.6173e-03,  0.0000e+00,  2.4354e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9966e-03,  0.0000e+00,\n",
      "         1.6528e-02,  0.0000e+00,  5.8060e-03, -1.7095e-02,  1.8633e-02,\n",
      "         0.0000e+00, -3.9447e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.3099e-02,  0.0000e+00,  0.0000e+00,  4.1572e-02,\n",
      "         0.0000e+00, -4.8810e-03,  0.0000e+00,  0.0000e+00,  1.2339e-02,\n",
      "         0.0000e+00,  0.0000e+00,  1.1836e-02,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0935e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5512e-02, -5.4740e-03,\n",
      "        -2.3557e-03, -3.3863e-02,  2.6404e-02, -9.6072e-03,  0.0000e+00,\n",
      "        -1.2875e-02,  1.5379e-03, -4.7370e-03,  0.0000e+00, -3.6655e-02,\n",
      "        -1.1195e-03, -1.7181e-02,  0.0000e+00, -1.0468e-02,  0.0000e+00,\n",
      "         2.7238e-03,  0.0000e+00, -4.2894e-02,  1.5355e-02,  4.7752e-02,\n",
      "         0.0000e+00,  0.0000e+00,  9.9028e-03,  4.8090e-03,  0.0000e+00,\n",
      "         1.3982e-02,  0.0000e+00,  1.1100e-02,  9.7569e-03,  1.7376e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.0245e-02, -1.0486e-02,  3.0951e-02,\n",
      "         0.0000e+00,  1.0421e-02,  0.0000e+00,  0.0000e+00, -7.9990e-03,\n",
      "         2.4930e-03,  0.0000e+00, -3.6926e-03, -2.5371e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  6.7729e-03, -2.8946e-02,  0.0000e+00,\n",
      "         1.0035e-02, -9.5142e-03,  0.0000e+00, -2.1165e-02,  1.3570e-02,\n",
      "         0.0000e+00, -3.7943e-02, -3.5855e-02,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -6.8719e-02, -2.2046e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -5.2797e-03,  0.0000e+00, -1.4562e-03,  0.0000e+00, -2.5415e-03,\n",
      "        -1.3140e-02,  0.0000e+00,  0.0000e+00,  2.7900e-03, -1.2856e-02,\n",
      "         7.1905e-03], device='cuda:0')\n",
      "tensor([[ 1.5108e-02,  3.8466e-03,  2.1268e-04,  ...,  2.9257e-02,\n",
      "          1.5865e-03,  9.0617e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-6.9086e-03, -1.7590e-03, -9.7252e-05,  ..., -1.3379e-02,\n",
      "         -7.2549e-04, -4.1437e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.6703e-03, -1.4437e-03, -7.9821e-05,  ..., -1.0981e-02,\n",
      "         -5.9545e-04, -3.4010e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "tensor([ 2.3371e-02,  0.0000e+00, -1.0687e-02,  4.1547e-02,  0.0000e+00,\n",
      "        -2.8483e-03,  2.6147e-02,  0.0000e+00,  1.1197e-02, -6.6910e-03,\n",
      "         0.0000e+00, -1.7390e-02, -2.8451e-02,  3.6085e-02,  0.0000e+00,\n",
      "         2.1816e-02,  8.3152e-03, -6.7736e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         2.5540e-02, -1.3949e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -8.9630e-03,  0.0000e+00, -3.3408e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0546e-02,  0.0000e+00,  1.2068e-02,\n",
      "         0.0000e+00, -6.0322e-03,  8.4062e-03,  1.8706e-03,  9.2079e-03,\n",
      "         0.0000e+00,  0.0000e+00,  4.1957e-02,  2.1800e-03,  0.0000e+00,\n",
      "        -1.5200e-02,  1.7502e-02,  1.1783e-02,  0.0000e+00, -4.8903e-02,\n",
      "        -9.2997e-03,  0.0000e+00,  8.6998e-03, -2.4046e-03,  2.5295e-02,\n",
      "         4.4600e-03,  0.0000e+00, -2.8579e-03,  0.0000e+00,  1.7355e-02,\n",
      "         0.0000e+00, -2.3380e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -3.6540e-02,  1.2376e-02,  3.0998e-02,  0.0000e+00,\n",
      "        -8.8045e-04, -4.6369e-02,  2.9557e-03,  0.0000e+00,  0.0000e+00,\n",
      "         7.0122e-03,  2.9520e-02, -3.1063e-02,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -7.5846e-03, -1.9405e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.4631e-02,  7.9873e-03,  0.0000e+00,\n",
      "         3.4426e-03,  0.0000e+00,  2.8896e-02,  0.0000e+00, -3.7019e-03,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.5670e-03, -1.0879e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.4571e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9440e-03,\n",
      "         4.6107e-03, -1.4603e-02,  0.0000e+00, -5.2043e-03,  1.6459e-02,\n",
      "         7.8702e-03,  0.0000e+00,  0.0000e+00,  9.0336e-03,  0.0000e+00,\n",
      "         6.2458e-03,  0.0000e+00,  3.5449e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7624e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.5315e-03,\n",
      "         0.0000e+00,  0.0000e+00, -1.5515e-02, -4.1994e-03, -1.8782e-03,\n",
      "         7.5996e-04, -3.6132e-02,  0.0000e+00,  2.2936e-02,  0.0000e+00,\n",
      "        -2.9064e-02,  5.4776e-03,  7.4618e-03,  2.1406e-02, -3.9726e-02,\n",
      "        -4.4208e-02, -2.0626e-02,  0.0000e+00,  0.0000e+00,  2.7986e-03,\n",
      "         2.2914e-02, -2.2801e-02,  2.5575e-02,  0.0000e+00,  3.5680e-02,\n",
      "         0.0000e+00,  2.4539e-03,  0.0000e+00,  1.8457e-04, -1.9559e-02,\n",
      "        -8.7261e-03,  0.0000e+00,  8.5130e-03, -3.6761e-03,  2.6997e-02,\n",
      "         2.3040e-03,  0.0000e+00, -1.5436e-02, -1.5012e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  4.9551e-03,  1.2809e-02, -1.6546e-02,\n",
      "         0.0000e+00,  3.8658e-03,  0.0000e+00, -1.3592e-02,  5.1687e-03,\n",
      "         0.0000e+00,  0.0000e+00,  2.2536e-02,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9609e-02,  3.2219e-03,\n",
      "        -2.2065e-02,  0.0000e+00,  1.3866e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -1.2685e-02,  0.0000e+00,  0.0000e+00, -2.0015e-03,\n",
      "         0.0000e+00,  0.0000e+00, -4.6968e-02, -1.6213e-02,  4.4629e-03,\n",
      "         0.0000e+00, -1.0812e-02,  5.0688e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.1757e-03,  2.3945e-02, -2.4874e-02,\n",
      "         0.0000e+00, -2.6793e-02,  0.0000e+00,  5.2309e-03,  4.6854e-03,\n",
      "         0.0000e+00,  0.0000e+00,  5.6694e-03,  0.0000e+00, -2.2174e-02,\n",
      "         0.0000e+00, -9.7664e-03,  0.0000e+00, -1.0005e-02,  0.0000e+00,\n",
      "         0.0000e+00,  3.9913e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.5003e-02, -3.9153e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5012e-03,\n",
      "        -3.8243e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.7716e-03,\n",
      "         0.0000e+00], device='cuda:0')\n",
      "tensor([[1.1500e-01, 0.0000e+00, 7.5401e-02, 5.1510e-02, 0.0000e+00, 2.9365e-01,\n",
      "         1.3689e-01, 0.0000e+00, 6.0664e-02, 2.2528e-01, 0.0000e+00, 1.8200e-01,\n",
      "         1.2930e-01, 8.3178e-02, 0.0000e+00, 1.1073e-01, 1.1422e-01, 2.2389e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.4030e-02, 2.0791e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.3892e-01, 0.0000e+00, 1.3579e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 6.2230e-02, 0.0000e+00, 1.8821e-01, 0.0000e+00, 1.2341e-02,\n",
      "         6.6812e-02, 2.5553e-01, 7.5196e-02, 0.0000e+00, 0.0000e+00, 1.9724e-01,\n",
      "         2.8043e-01, 0.0000e+00, 5.4885e-02, 2.3237e-01, 4.0376e-02, 0.0000e+00,\n",
      "         1.5472e-01, 1.8121e-01, 0.0000e+00, 1.1720e-01, 3.1143e-02, 3.4494e-02,\n",
      "         3.1164e-01, 0.0000e+00, 1.8515e-02, 0.0000e+00, 2.4744e-01, 0.0000e+00,\n",
      "         2.3807e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1946e-02,\n",
      "         3.5206e-04, 1.1391e-01, 0.0000e+00, 8.0449e-03, 1.2098e-01, 3.7014e-01,\n",
      "         0.0000e+00, 0.0000e+00, 2.0006e-01, 3.2285e-01, 1.0353e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5175e-02, 5.0056e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.3886e-01, 1.4994e-01, 0.0000e+00, 3.2740e-02,\n",
      "         0.0000e+00, 1.0602e-01, 0.0000e+00, 9.1024e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0193e-01, 4.0192e-02, 0.0000e+00, 0.0000e+00, 1.1076e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9191e-01, 2.4840e-02, 8.5071e-02, 0.0000e+00, 6.4752e-02, 4.6940e-01,\n",
      "         4.3793e-02, 0.0000e+00, 0.0000e+00, 2.6335e-01, 0.0000e+00, 8.1106e-02,\n",
      "         0.0000e+00, 2.7732e-02, 0.0000e+00, 0.0000e+00, 3.8995e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.7494e-02, 0.0000e+00, 0.0000e+00, 1.4039e-01,\n",
      "         2.0809e-02, 3.2251e-01, 5.4072e-02, 4.1896e-02, 0.0000e+00, 2.9092e-02,\n",
      "         0.0000e+00, 1.3007e-01, 7.9619e-02, 2.3861e-02, 1.1123e-01, 1.9198e-01,\n",
      "         3.4933e-01, 1.2375e-01, 0.0000e+00, 0.0000e+00, 6.2548e-03, 7.9223e-02,\n",
      "         9.0924e-02, 1.6718e-01, 0.0000e+00, 2.6427e-01, 0.0000e+00, 4.6166e-02,\n",
      "         0.0000e+00, 3.1579e-02, 3.0494e-02, 2.1539e-01, 0.0000e+00, 2.1444e-02,\n",
      "         8.3454e-03, 2.4149e-01, 5.6854e-02, 0.0000e+00, 3.7698e-02, 1.6002e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5049e-02, 7.3347e-02, 1.7261e-01,\n",
      "         0.0000e+00, 1.1266e-01, 0.0000e+00, 9.9232e-02, 6.5349e-02, 0.0000e+00,\n",
      "         0.0000e+00, 4.5151e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.6443e-01, 1.0802e-01, 7.0702e-02, 0.0000e+00, 8.3552e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8844e-03, 0.0000e+00, 0.0000e+00,\n",
      "         1.5984e-01, 0.0000e+00, 0.0000e+00, 1.0507e-01, 4.0271e-02, 7.7102e-02,\n",
      "         0.0000e+00, 2.0932e-01, 1.9966e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.5316e-01, 6.3511e-02, 8.5960e-02, 0.0000e+00, 7.2015e-02,\n",
      "         0.0000e+00, 2.7617e-01, 2.7237e-01, 0.0000e+00, 0.0000e+00, 1.5639e-01,\n",
      "         0.0000e+00, 1.1022e-01, 0.0000e+00, 3.5544e-01, 0.0000e+00, 6.7752e-02,\n",
      "         0.0000e+00, 0.0000e+00, 9.8134e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7414e-01, 3.3670e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7074e-01, 1.9826e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.1784e-02, 0.0000e+00],\n",
      "        [2.3091e-02, 0.0000e+00, 1.5140e-02, 1.0343e-02, 0.0000e+00, 5.8962e-02,\n",
      "         2.7487e-02, 0.0000e+00, 1.2181e-02, 4.5234e-02, 0.0000e+00, 3.6545e-02,\n",
      "         2.5962e-02, 1.6701e-02, 0.0000e+00, 2.2233e-02, 2.2935e-02, 4.4955e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 6.8329e-03, 4.1747e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7973e-02, 0.0000e+00, 2.7266e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.2495e-02, 0.0000e+00, 3.7790e-02, 0.0000e+00, 2.4779e-03,\n",
      "         1.3415e-02, 5.1307e-02, 1.5099e-02, 0.0000e+00, 0.0000e+00, 3.9604e-02,\n",
      "         5.6307e-02, 0.0000e+00, 1.1020e-02, 4.6658e-02, 8.1071e-03, 0.0000e+00,\n",
      "         3.1067e-02, 3.6386e-02, 0.0000e+00, 2.3532e-02, 6.2532e-03, 6.9262e-03,\n",
      "         6.2574e-02, 0.0000e+00, 3.7177e-03, 0.0000e+00, 4.9683e-02, 0.0000e+00,\n",
      "         4.7802e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3986e-03,\n",
      "         7.0691e-05, 2.2873e-02, 0.0000e+00, 1.6153e-03, 2.4292e-02, 7.4321e-02,\n",
      "         0.0000e+00, 0.0000e+00, 4.0170e-02, 6.4825e-02, 2.0788e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3087e-02, 1.0051e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.7883e-02, 3.0108e-02, 0.0000e+00, 6.5739e-03,\n",
      "         0.0000e+00, 2.1288e-02, 0.0000e+00, 1.8277e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.0467e-02, 8.0701e-03, 0.0000e+00, 0.0000e+00, 2.2240e-03,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.8534e-02, 4.9877e-03, 1.7081e-02, 0.0000e+00, 1.3002e-02, 9.4251e-02,\n",
      "         8.7931e-03, 0.0000e+00, 0.0000e+00, 5.2879e-02, 0.0000e+00, 1.6285e-02,\n",
      "         0.0000e+00, 5.5683e-03, 0.0000e+00, 0.0000e+00, 7.8299e-03, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.1544e-02, 0.0000e+00, 0.0000e+00, 2.8189e-02,\n",
      "         4.1783e-03, 6.4758e-02, 1.0857e-02, 8.4124e-03, 0.0000e+00, 5.8414e-03,\n",
      "         0.0000e+00, 2.6116e-02, 1.5987e-02, 4.7910e-03, 2.2334e-02, 3.8549e-02,\n",
      "         7.0142e-02, 2.4848e-02, 0.0000e+00, 0.0000e+00, 1.2559e-03, 1.5907e-02,\n",
      "         1.8257e-02, 3.3568e-02, 0.0000e+00, 5.3063e-02, 0.0000e+00, 9.2696e-03,\n",
      "         0.0000e+00, 6.3407e-03, 6.1229e-03, 4.3249e-02, 0.0000e+00, 4.3057e-03,\n",
      "         1.6757e-03, 4.8488e-02, 1.1416e-02, 0.0000e+00, 7.5693e-03, 3.2131e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5069e-02, 1.4727e-02, 3.4658e-02,\n",
      "         0.0000e+00, 2.2622e-02, 0.0000e+00, 1.9925e-02, 1.3121e-02, 0.0000e+00,\n",
      "         0.0000e+00, 9.0660e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.3095e-02, 2.1689e-02, 1.4196e-02, 0.0000e+00, 1.6776e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7839e-03, 0.0000e+00, 0.0000e+00,\n",
      "         3.2093e-02, 0.0000e+00, 0.0000e+00, 2.1097e-02, 8.0860e-03, 1.5481e-02,\n",
      "         0.0000e+00, 4.2030e-02, 4.0091e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.0753e-02, 1.2752e-02, 1.7260e-02, 0.0000e+00, 1.4460e-02,\n",
      "         0.0000e+00, 5.5453e-02, 5.4689e-02, 0.0000e+00, 0.0000e+00, 3.1401e-02,\n",
      "         0.0000e+00, 2.2131e-02, 0.0000e+00, 7.1370e-02, 0.0000e+00, 1.3604e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.9704e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.4965e-02, 6.7605e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4283e-02, 3.9809e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.3741e-03, 0.0000e+00]], device='cuda:0')\n",
      "tensor([0.2015, 0.0405], device='cuda:0')\n",
      "Grad norm of actor: 3.2709999084472656\n"
     ]
    }
   ],
   "source": [
    "random_state = torch.rand(1, state_size).to(device)\n",
    "\n",
    "action = actor(random_state)\n",
    "\n",
    "loss_actor = -critic.Q1_forward(random_state, action).mean()\n",
    "\n",
    "print(f'loss_actor: {loss_actor}')\n",
    "\n",
    "actor_optimizer.zero_grad()\n",
    "loss_actor.backward()\n",
    "for p in actor.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "print(f'Grad norm of actor: {torch.norm(torch.cat([p.grad.flatten() for p in actor.parameters()]))}')\n",
    "\n",
    "actor_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "x = [1,2]\n",
    "y = [3,4]\n",
    "\n",
    "# Combine two lists into one\n",
    "combine = lambda x, y: [i for j in zip(x, y) for i in j]\n",
    "z = combine(x, y)\n",
    "\n",
    "print(list(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
